Image denoising using U-Net
===

>## Introduction
* We implement various open cnn models for denoising tasks.
* We improve denoising performance by transforming the models.

>## U-Net

![U-Net](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/unet.png)
* The U-net has a u-shaped network structure, and the network on the left and right sides is symmetrically formed. 
* Based on the center of the u-shaped network, the left side is defined as the contracting path and the right side as the expanding path. 
* The contracting path helps capture the context of the input, and the expanding path do up-sampling the feature map and combines it with the context of the feature map captured in the contexting path for more accurate localization.

### Contracting path
![Contracting path](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/contracting_path.png)
* The contraction path repeats the 3by3 convolution twice for each step
* It contains a relu as an activation function, and performs a 2by2 max-pooling operation for each step to halve the size of the feature map. 
* At this point, the number of channels doubles for each down-sampling.

### Expanding path
![Expanding path](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/expanding_path.png)
* The expanding path is a configuration for fine localization and extends the feature map to the opposite operation of the contraction path. 
* It performs a 2by2 up-convolution for each step, doubling the size of the feature map, and halving the number of channels for each up-sampling. 
* In addition, repeat the 3by3 convolution twice for each step, just like the contraction path.

### Skip connection
![Skip connection](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/skip_connection.png)
* Skip connection combines the up-convolution feature map with the feature map of the contraction path for each step. 
* This passes important information from the contracting path to the expanding path. 
* As a result, the expanding path can obtain a clearer image.

>## Method

### Residual layer
![Residual layer](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/residual.png)
* The plain layer is newly generated by transforming previously learned information without preserving it.
* On the other hand, the residual layer benefit learning performance by preserving and additionally learning information that has been learned before.

### Normalization
![Normalization](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/normalization.png)
* Batch normalization is used to speed up learning and reduce the likelihood of falling into the local optimization problem.
* Unlike batch normalization, where one channel is normalized by finding means and variances, instance normalization is normalized by finding means and variances for one channel and one batch.

### Activation
![Activation](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/activation.png)
* The activation function uses the ReLU and Leaky ReLU functions.
* ReLU: Values below 0 are not passed to the next layer. Output values above zero as they are.
* Leaky ReLU: Transfers less than zero to the next layer in small quantities.

### Loss
![Loss](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/loss.png)
* Using MSE loss function and loss function that combines L1 and MS-SSIM.

### Learning rate scheduler
![Learning rate scheduler](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/scheduler.png)
* Using LambdaLR scheduler to adjust the learning rate.
* LambdaLR regulates the learning rate through a function written by the lambda expression.

>## Experiment

### Compare plain/residual layer
* Case #1. – BN + ReLU + L1 (+MS-SSIM Loss)
![case #1](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/compare1.png)
![result #1](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/result1.png)

* Case #2. – BN + ReLU + L2 loss
![case #2](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/compare2.png)
![result #2](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/result2.png)

### Compare normalization
* Case #1. – ReLU + Residual + L1 (+MS-SSIM Loss)
![case #1](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/compare3.png)
![result #1](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/result3.png)

* Case #2. – ReLU + Residual + L2 loss
![case #2](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/compare4.png)
![result #2](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/result4.png)

### Compare activation
* Case #1. – BN + Residual + L1 (+MS-SSIM Loss)
![case #1](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/compare5.png)
![result #1](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/result5.png)

* Case #2. – IN + Residual + L2 loss
![case #2](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/compare6.png)
![result #2](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/result6.png)

### Compare loss
* Case # – BN + ReLU + Residual 
![case](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/compare7.png)
![result](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/result7.png)

### Compare batch size
* Case # – BN + ReLU + Residual + L2 loss
![case](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/compare7.png)
![result](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/result7.png)
![result](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/result8.png)

>## Conclusion

* Model: U-Net
* Optimizer: Adam optimizer
* Learning rate: 1e-3

### Best combination
* Using L1 + MS-SSIM loss
![combination](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/combination1.png)
* Using L2 loss
![combination](https://github.com/JunHeon-Ch/Image_Denoising_UNet/blob/main/image/combination2.png)
